{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiQ5eDr0PqJk",
        "outputId": "45347eab-c273-44be-b124-b90d2391646c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 40078, number of negative: 39922\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006315 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500975 -> initscore=0.003900\n",
            "[LightGBM] [Info] Start training from score 0.003900\n",
            "Best parameters: {'lgbm__learning_rate': 0.1, 'lgbm__max_depth': -1, 'lgbm__n_estimators': 200, 'lgbm__num_leaves': 50}\n",
            "Best cross-validation F1 score: 0.9947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score on validation set: 0.9953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 50000, number of negative: 50000\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008704 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 9\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Submission file 'submission.csv' created successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv(\"Train_Data.csv\")\n",
        "test_data = pd.read_csv(\"Test_Data.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = train_data.drop(columns=[\"Sepssis\"])\n",
        "y = train_data[\"Sepssis\"]  # 'Negative' or 'Positive'\n",
        "\n",
        "# Convert target to binary (LightGBM needs numeric labels for training)\n",
        "y = y.map({'Negative': 0, 'Positive': 1})\n",
        "\n",
        "# Split for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data (LightGBM handles categories natively, but we impute missing)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\"))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_cols),\n",
        "        (\"cat\", categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Full pipeline with preprocessing and LightGBM\n",
        "pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"lgbm\", LGBMClassifier(random_state=42, objective=\"binary\"))\n",
        "])\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"lgbm__n_estimators\": [100, 200],\n",
        "    \"lgbm__learning_rate\": [0.01, 0.1],\n",
        "    \"lgbm__max_depth\": [3, 5, -1],  # -1 means no limit\n",
        "    \"lgbm__num_leaves\": [31, 50]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"f1\", n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train, lgbm__categorical_feature=categorical_cols.tolist())\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "print(f\"F1 Score on validation set: {f1:.4f}\")\n",
        "\n",
        "# Train on full dataset\n",
        "best_model.fit(X, y, lgbm__categorical_feature=categorical_cols.tolist())\n",
        "\n",
        "# Predict on test set\n",
        "test_predictions = best_model.predict(test_data)\n",
        "\n",
        "# Convert predictions back to 'Negative'/'Positive' for submission\n",
        "test_predictions = np.where(test_predictions == 0, 'Negative', 'Positive')\n",
        "\n",
        "# Prepare submission\n",
        "submission_df = pd.DataFrame({\"Sepssis\": test_predictions})\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
